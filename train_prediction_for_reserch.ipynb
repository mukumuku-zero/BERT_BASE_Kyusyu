{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf070da-2a9b-46f6-a6fd-be02506dc054",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def text_fix(df, use_col):\n",
    "    for col in use_col:\n",
    "        df[col] = df[col].apply(lambda x: x.replace('\\n', '。'))\n",
    "        df[col] = df[col].apply(lambda x: re.sub(r'。+', '。', x))\n",
    "        df[col] = df[col].apply(lambda x: x.replace('　', ''))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315063c9-e7ef-43c3-838d-b7fae9b6425d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers.trainer_utils import set_seed\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_curve, roc_auc_score\n",
    "from pprint import pprint\n",
    "from datasets import Dataset\n",
    "from typing import Union\n",
    "from transformers import BatchEncoding, EarlyStoppingCallback\n",
    "from collections import Counter\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gc\n",
    "import japanize_matplotlib\n",
    "import shap\n",
    "from transformers import TrainerCallback, TrainingArguments, Trainer\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def __init__(self, *args, criterion=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.criterion = criterion\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")[:, 1]  # positiveクラスのロジットのみを取得\n",
    "        loss = self.criterion(logits, labels.float())\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "    # save_modelをオーバーライドして、保存前に .contiguous() を適用\n",
    "    def save_model(self, output_dir=None, _internal_call=False):\n",
    "        # モデルの全パラメータに対して .contiguous() を適用\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if not param.is_contiguous():\n",
    "                param.data = param.contiguous()  # 非連続テンソルを連続に変換\n",
    "\n",
    "        # 通常の保存処理を呼び出す\n",
    "        super().save_model(output_dir, _internal_call=_internal_call)\n",
    "\n",
    "def set_random_seed(seed: int = 42):\n",
    "    set_seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "    # 再現性を保つための設定\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    print(\"乱数シード設定完了\")\n",
    "\n",
    "def report_memory():\n",
    "    print(f\"Allocated: {torch.cuda.memory_allocated() / 1024 ** 2:.2f} MiB\")\n",
    "    print(f\"Cached: {torch.cuda.memory_reserved() / 1024 ** 2:.2f} MiB\")\n",
    "\n",
    "def cleanup_gpu_memory():\n",
    "    \"\"\"\n",
    "    GPUキャッシュを空にし、CUDAメモリをリセットし、メモリ使用状況を表示する関数。\n",
    "    \"\"\"\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    print(\"After cleanup:\")\n",
    "    report_memory()\n",
    "\n",
    "class ContiguousCallback(TrainerCallback):\n",
    "    def on_save(self, args, state, control, **kwargs):\n",
    "        # モデルの全パラメータに対して .contiguous() を適用\n",
    "        model = kwargs['model']\n",
    "        for name, param in model.named_parameters():\n",
    "            if not param.is_contiguous():\n",
    "                param.data = param.contiguous()\n",
    "        return control\n",
    "\n",
    "# def load_data(train_path: str, valid_path: str):\n",
    "def load_data(original_train_df, valid_df):\n",
    "    train_dataset = Dataset.from_pandas(original_train_df)\n",
    "    valid_dataset = Dataset.from_pandas(valid_df)\n",
    "    pprint(train_dataset[0])\n",
    "    return train_dataset, valid_dataset\n",
    "\n",
    "def tokenize_data(train_dataset, valid_dataset, col, model_name: str):\n",
    "    set_random_seed(42)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokens = tokenizer.tokenize(train_dataset[0][col])\n",
    "    print(tokens)\n",
    "\n",
    "    def preprocess_text_classification(example: dict[str, Union[str, int]]) -> BatchEncoding:\n",
    "        # トークナイザーの最大シーケンス長を取得\n",
    "        max_length = tokenizer.model_max_length\n",
    "        try:\n",
    "            encoded_example = tokenizer(example[col], max_length=max_length, truncation=True, padding='longest')\n",
    "            encoded_example[\"labels\"] = example[\"obj\"]\n",
    "            return encoded_example\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing example: {example}\")\n",
    "            print(f\"Error message: {e}\")\n",
    "            raise e\n",
    "\n",
    "    try:\n",
    "        encoded_train_dataset = train_dataset.map(preprocess_text_classification, remove_columns=train_dataset.column_names)\n",
    "        encoded_valid_dataset = valid_dataset.map(preprocess_text_classification, remove_columns=valid_dataset.column_names)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during dataset mapping: {e}\")\n",
    "        raise e\n",
    "\n",
    "    print(encoded_train_dataset[0])\n",
    "    return encoded_train_dataset, encoded_valid_dataset, tokenizer\n",
    "\n",
    "# モデルの保存時に .contiguous() を適用するカスタム関数\n",
    "def save_model_with_contiguous(model, save_directory):\n",
    "    for name, param in model.named_parameters():\n",
    "        if not param.is_contiguous():\n",
    "            param.data = param.contiguous()  # 非連続なテンソルを連続に変換\n",
    "    model.save_pretrained(save_directory)  # 通常の保存処理\n",
    "\n",
    "def prepare_model(model_name: str, num_labels: int):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = (AutoModelForSequenceClassification\n",
    "             .from_pretrained(model_name, num_labels=num_labels)\n",
    "             .to(device))\n",
    "    return model\n",
    "\n",
    "def prepare_trainer(model, encoded_train_dataset, encoded_valid_dataset, tokenizer, epoch, samples_per_class, num_labels, ratio, output_dir: str):\n",
    "    set_random_seed(42)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    pos_weight_tensor = torch.tensor([ratio]).to(device)\n",
    "\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        learning_rate=2e-5,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        warmup_ratio=0.1,\n",
    "        num_train_epochs=epoch,\n",
    "        save_strategy=\"epoch\", # エポックごとに保存\n",
    "        save_total_limit=1, # 最新の1つだけを保存\n",
    "        logging_strategy=\"epoch\", # エポックごとにログ保存\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"ROC_AUC\",\n",
    "        fp16=True,\n",
    "    )\n",
    "\n",
    "    def compute_metrics(eval_pred):\n",
    "        predictions, labels = eval_pred\n",
    "        predictions = np.argmax(predictions, axis=1)\n",
    "        accuracy = accuracy_score(labels, predictions)\n",
    "        precision_weighted = precision_score(labels, predictions, average='weighted')\n",
    "        recall_weighted = recall_score(labels, predictions, average='weighted')\n",
    "        f1_weighted = f1_score(labels, predictions, average='weighted')\n",
    "        precision_macro = precision_score(labels, predictions, average='macro')\n",
    "        recall_macro = recall_score(labels, predictions, average='macro')\n",
    "        f1_macro = f1_score(labels, predictions, average='macro')\n",
    "        roc_auc = roc_auc_score(labels, predictions)\n",
    "        return {\"accuracy\": accuracy, \"precision_macro\": precision_macro, \"recall_macro\": recall_macro, \"f1_macro\": f1_macro, \"precision_weighted\": precision_weighted, \"recall_weighted\": recall_weighted, \"f1_weighted\": f1_weighted, 'ROC_AUC': roc_auc}\n",
    "\n",
    "    trainer = CustomTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=encoded_train_dataset,\n",
    "        eval_dataset=encoded_valid_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)], # add early stopping\n",
    "        criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight_tensor)\n",
    "    )\n",
    "    \n",
    "    return trainer\n",
    "\n",
    "def train_and_evaluate(trainer):\n",
    "    set_random_seed(42)\n",
    "    trainer.train()\n",
    "    return trainer\n",
    "\n",
    "def save_predictions(trainer, encoded_valid_dataset, valid_dataset, col, output_file: str):\n",
    "    set_random_seed(42)\n",
    "    predictions = trainer.predict(encoded_valid_dataset)\n",
    "    predictions_df = pd.DataFrame({\n",
    "        'obj': predictions.label_ids,\n",
    "        'predicted_label': predictions.predictions.argmax(axis=1),\n",
    "        'post': valid_dataset[col]\n",
    "    })\n",
    "    predictions_df.to_csv(output_file, index=False)\n",
    "\n",
    "def evaluate_predictions(predictions_df, output_file: str):\n",
    "    conf_matrix = confusion_matrix(predictions_df['obj'], predictions_df['predicted_label'])\n",
    "    unique_labels = sorted(set(predictions_df['obj'].unique()) | set(predictions_df['predicted_label'].unique()))\n",
    "    conf_matrix_df = pd.DataFrame(conf_matrix, columns=unique_labels, index=unique_labels)\n",
    "    conf_matrix_df.to_csv(output_file)\n",
    "\n",
    "    accuracy = accuracy_score(predictions_df['obj'], predictions_df['predicted_label'])\n",
    "    precision = precision_score(predictions_df['obj'], predictions_df['predicted_label'], average='macro')\n",
    "    recall = recall_score(predictions_df['obj'], predictions_df['predicted_label'], average='macro')\n",
    "    roc_auc = roc_auc_score(predictions_df['obj'], predictions_df['predicted_label'])\n",
    "\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\", recall)\n",
    "    print(\"ROC_AUC:\", roc_auc)\n",
    "\n",
    "def llm_classification(MODEL_NAME, original_train_df, valid_df, epoch, model_dir, device, type, col):\n",
    "    set_random_seed(42)\n",
    "\n",
    "    print(\"Initial memory usage:\")\n",
    "    report_memory()\n",
    "\n",
    "    dir = f\"results/{type}/{col}\"\n",
    "    if not os.path.exists(dir): # ディレクトリが存在するか確認\n",
    "        os.makedirs(dir)\n",
    "\n",
    "    original_train_df = original_train_df.reset_index(drop=True)\n",
    "    valid_df = valid_df.reset_index(drop=True)\n",
    "\n",
    "    train_dataset, valid_dataset = load_data(original_train_df, valid_df)\n",
    "    encoded_train_dataset, encoded_valid_dataset, tokenizer = tokenize_data(train_dataset, valid_dataset, col, MODEL_NAME)\n",
    "\n",
    "    # クラスごとのサンプル数を計算\n",
    "    labels = train_dataset[\"obj\"]\n",
    "    num_classes = len(set(labels))\n",
    "    print('num_classes:', num_classes)\n",
    "    samples_per_class = [labels.count(i) for i in range(num_classes)]\n",
    "    print('samples_per_class')\n",
    "    print(samples_per_class)\n",
    "    \n",
    "    labels = [example[\"obj\"] for example in train_dataset]\n",
    "    num_labels = np.max(labels) + 1\n",
    "    print('num_labels:', num_labels)\n",
    "\n",
    "    model = prepare_model(MODEL_NAME, num_labels)\n",
    "    # テンソルを連続化\n",
    "    for param in model.parameters():\n",
    "        if not param.is_contiguous():\n",
    "            param.data = param.data.contiguous()\n",
    "\n",
    "    ratio = np.sum(original_train_df['obj'].values==0)/np.sum(original_train_df['obj'].values==1)\n",
    "    trainer = prepare_trainer(model, encoded_train_dataset, encoded_valid_dataset, tokenizer, epoch, samples_per_class, num_labels, ratio, \"output_wrime\")\n",
    "    trainer = train_and_evaluate(trainer)\n",
    "    save_predictions(trainer, encoded_valid_dataset, valid_dataset, col, f\"{dir}/results_lmm_{type}_{col}.csv\")\n",
    "\n",
    "    predictions_df = pd.read_csv(f\"{dir}/results_lmm_{type}_{col}.csv\")\n",
    "    evaluate_predictions(predictions_df, f\"{dir}/confusion_matrix_llm_{type}_{col}.csv\")\n",
    "\n",
    "    # モデルの評価\n",
    "    trainer.evaluate()\n",
    "\n",
    "    ## 学習曲線の表示 ##\n",
    "\n",
    "    # 学習曲線の保存\n",
    "    # ログファイルから学習曲線のデータを取得\n",
    "    logs = trainer.state.log_history\n",
    "\n",
    "    # 学習と検証の損失をプロット\n",
    "    train_loss = [log['loss'] for log in logs if 'loss' in log]\n",
    "    eval_loss = [log['eval_loss'] for log in logs if 'eval_loss' in log]\n",
    "\n",
    "    plt.plot(train_loss, label='Train Loss')\n",
    "    plt.plot(eval_loss, label='Eval Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.title('Training and Evaluation Loss')\n",
    "\n",
    "    # 図を表示\n",
    "    plt.show()\n",
    "\n",
    "    # モデルとトークナイズの保存\n",
    "    model.save_pretrained(model_dir)\n",
    "    tokenizer.save_pretrained(model_dir)\n",
    "\n",
    "    ################################################################################################\n",
    "    del model, tokenizer, trainer\n",
    "    cleanup_gpu_memory()\n",
    "    ################################################################################################\n",
    "\n",
    "def make_vector_data(model, tokenizer, df, device, dir, kind, type, col):\n",
    "    \n",
    "    # 100行ずつ実行\n",
    "    num_rows_per_df = len(df)\n",
    "    dfs = [df.iloc[i:i + num_rows_per_df] for i in range(0, len(df), num_rows_per_df)]\n",
    "    \n",
    "    print('推論開始')\n",
    "    \n",
    "    # 処理後のデータフレームを保存するリスト\n",
    "    processed_dfs = []\n",
    "    submit_dfs = []\n",
    "\n",
    "    max_length = tokenizer.model_max_length\n",
    "\n",
    "    print('model_max_length:', max_length)\n",
    "    \n",
    "    for i, df_part in enumerate(dfs):\n",
    "        set_random_seed(42)\n",
    "    \n",
    "        df_part = df_part.reset_index(drop=True)\n",
    "        \n",
    "        data_list = df_part[col].values.tolist()\n",
    "        inputs = tokenizer(data_list, return_tensors='pt', max_length=max_length, truncation=True, padding='longest')\n",
    "    \n",
    "        # 入力データをモデルと同じデバイスに転送\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            set_random_seed(42)\n",
    "            outputs = model(**inputs, output_hidden_states=True)\n",
    "            final_layer_vectors = outputs[\"hidden_states\"][-1]  # 最終層のベクトルを取得\n",
    "            print(f\"Shape of final_layer_vectors_{kind}: {final_layer_vectors.shape}\")\n",
    "    \n",
    "        # mean_vectors = final_layer_vectors.mean(dim=1).cpu().numpy()\n",
    "        final_layer_vectors = final_layer_vectors.mean(dim=1).cpu().numpy()\n",
    "        column_names = [f'dim{i}' for i in range(final_layer_vectors.shape[1])]\n",
    "        df_vec = pd.DataFrame(final_layer_vectors, columns=column_names).reset_index(drop=True)\n",
    "\n",
    "        df_vec['pid'] = df_part['pid']\n",
    "        df_vec['hospitalization_date'] = df_part['hospitalization_date']\n",
    "        df_vec['discharge_data'] = df_part['discharge_data']\n",
    "        df_vec['train_validation_test'] = df_part['train_validation_test']\n",
    "        df_vec['obj'] = df_part['obj']\n",
    "    \n",
    "        ################################################################################################\n",
    "        del final_layer_vectors\n",
    "        cleanup_gpu_memory()\n",
    "        ################################################################################################\n",
    "    \n",
    "        \n",
    "        # 予測結果取得\n",
    "        logits = outputs.logits\n",
    "        pred = F.softmax(logits, dim=-1)\n",
    "        df_pred = pd.DataFrame(pred.cpu().numpy(), columns=['Pred_class_0', 'Pred_class_1']).reset_index(drop=True)\n",
    "    \n",
    "        result_df = pd.DataFrame(pred.cpu().numpy().argmax(axis=1), columns=['obj_pred']).reset_index(drop=True)\n",
    "    \n",
    "        df_merged = pd.concat([result_df, df_pred], axis=1)\n",
    "        df_merged = pd.concat([df_merged, df_vec], axis=1)\n",
    "        submit_dfs.append(df_merged)\n",
    "    \n",
    "        ################################################################################################\n",
    "        del logits, outputs, pred, df_pred, result_df, df_part, df_merged, df_vec, inputs, data_list\n",
    "        cleanup_gpu_memory()\n",
    "        ################################################################################################\n",
    "\n",
    "    df_submit = pd.concat(submit_dfs, ignore_index=True)\n",
    "    display(df_submit.head())\n",
    "    df_submit.to_csv(f'{dir}/BERT_vector_{kind}_{type}_{col}.csv', index=False, float_format='%.30f')\n",
    "    print(f'{dir}/BERT_vector_{kind}_{type}_{col}.csvの保存が完了しました')\n",
    "    \n",
    "    ################################################################################################\n",
    "    del submit_dfs, df_submit, dfs\n",
    "    cleanup_gpu_memory()\n",
    "    ################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28587b45-39fc-4def-b9b5-c79c437eabbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "MODEL_NAME = 'tohoku-nlp/bert-base-japanese-whole-word-masking'\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('device:', device)\n",
    "\n",
    "use_col = ['初診時記録_身体所見', 'ER現病歴_身体所見']\n",
    "\n",
    "train_mRS3_5 = df_mRS3_5[df_mRS3_5['train_validation_test']=='train']\n",
    "valid_mRS3_5 = df_mRS3_5[df_mRS3_5['train_validation_test']=='valid']\n",
    "test_mRS3_5 = df_mRS3_5[df_mRS3_5['train_validation_test']=='test']\n",
    "\n",
    "train_mRS3_6 = df_mRS3_6[df_mRS3_6['train_validation_test']=='train']\n",
    "valid_mRS3_6 = df_mRS3_6[df_mRS3_6['train_validation_test']=='valid']\n",
    "test_mRS3_6 = df_mRS3_6[df_mRS3_6['train_validation_test']=='test']\n",
    "\n",
    "train_mRS6 = df_mRS6[df_mRS6['train_validation_test']=='train']\n",
    "valid_mRS6 = df_mRS6[df_mRS6['train_validation_test']=='valid']\n",
    "test_mRS6 = df_mRS6[df_mRS6['train_validation_test']=='test']\n",
    "\n",
    "print('データ準備完了')\n",
    "\n",
    "# epoch数\n",
    "epoch = 10\n",
    "\n",
    "type_list = ['mRS3_5', 'mRS3_6', 'mRS6']\n",
    "\n",
    "for type in type_list:\n",
    "    for col in use_col:\n",
    "        print('type:', type)\n",
    "        print('use_text:', col)\n",
    "        model_dir = f\"models/{type}/{col}/BERT_{type}_{col}\"\n",
    "        \n",
    "        if type == 'mRS3_5':\n",
    "            train_data = train_mRS3_5.copy()\n",
    "            val_data = valid_mRS3_5.copy()\n",
    "            test_data = test_mRS3_5.copy()\n",
    "        elif type == 'mRS3_6':\n",
    "            train_data = train_mRS3_6.copy()\n",
    "            val_data = valid_mRS3_6.copy()\n",
    "            test_data = test_mRS3_6.copy()\n",
    "        else:\n",
    "            train_data = train_mRS6.copy()\n",
    "            val_data = valid_mRS6.copy()\n",
    "            test_data = test_mRS6.copy()\n",
    "\n",
    "        lm_classification(MODEL_NAME, train_data, val_data, epoch, model_dir, device, type, col)\n",
    "\n",
    "        num_labels = 2\n",
    "        dir = f\"results/{type}/{col}\"\n",
    "        \n",
    "        model = (AutoModelForSequenceClassification\n",
    "              .from_pretrained(model_dir, num_labels=num_labels)\n",
    "              .to(device))\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "        \n",
    "        # テストデータに対して推論\n",
    "        model.eval()\n",
    "        set_random_seed(42)\n",
    "        make_vector_data(model, tokenizer, train_data, device, dir, 'train', type, col)\n",
    "        make_vector_data(model, tokenizer, val_data, device, dir, 'valid', type, col)\n",
    "        make_vector_data(model, tokenizer, test_data, device, dir, 'test', type, col)\n",
    "        \n",
    "        ################################################################################################\n",
    "        del model, tokenizer\n",
    "        cleanup_gpu_memory()\n",
    "        ################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afea98d-27bd-4c29-9091-1480e1a64ce5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
